"""
BarbellGPT ä¸»ç¨‹åº

åŠ›é‡ä¸¾è®­ç»ƒæ™ºèƒ½åŠ©æ‰‹çš„ä¸»ç¨‹åºå…¥å£ã€‚
"""

import os
import sys
from pathlib import Path
from loguru import logger

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

def setup_logging():
    """è®¾ç½®æ—¥å¿—é…ç½®"""
    from config import LOG_LEVEL, LOG_FILE
    
    # é…ç½®loguru
    logger.remove()  # ç§»é™¤é»˜è®¤å¤„ç†å™¨
    
    # æ·»åŠ æ§åˆ¶å°å¤„ç†å™¨
    logger.add(
        sys.stderr,
        level=LOG_LEVEL,
        format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>"
    )
    
    # æ·»åŠ æ–‡ä»¶å¤„ç†å™¨
    logger.add(
        LOG_FILE,
        level=LOG_LEVEL,
        format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} - {message}",
        rotation="10 MB",
        retention="7 days"
    )

def check_dependencies():
    """æ£€æŸ¥ä¾èµ–æ˜¯å¦å®‰è£…"""
    required_packages = [
        'streamlit', 'langchain', 'langchain_community', 
        'chromadb', 'sentence_transformers', 'loguru'
    ]
    
    missing_packages = []
    for package in required_packages:
        try:
            __import__(package)
        except ImportError:
            missing_packages.append(package)
    
    if missing_packages:
        logger.error(f"ç¼ºå°‘ä¾èµ–åŒ…: {', '.join(missing_packages)}")
        logger.error("è¯·è¿è¡Œ: pip install -r requirements.txt")
        return False
    
    logger.info("æ‰€æœ‰ä¾èµ–åŒ…æ£€æŸ¥é€šè¿‡")
    return True

def check_config():
    """æ£€æŸ¥é…ç½®"""
    from config import DASHSCOPE_API_KEY
    
    if not DASHSCOPE_API_KEY:
        logger.warning("æœªè®¾ç½®DASHSCOPE_API_KEYï¼ŒLLMåŠŸèƒ½å°†ä¸å¯ç”¨")
        logger.info("è¯·åœ¨ç¯å¢ƒå˜é‡ä¸­è®¾ç½®DASHSCOPE_API_KEY")
        return False
    
    logger.info("é…ç½®æ£€æŸ¥é€šè¿‡")
    return True

def create_sample_data():
    """åˆ›å»ºç¤ºä¾‹æ•°æ®"""
    from knowledge.document_loader import DocumentLoader
    from knowledge.text_processor import TextProcessor
    from agents.rag_agent import RAGAgent
    
    try:
        # æ£€æŸ¥æ˜¯å¦æœ‰æ–‡æ¡£
        loader = DocumentLoader()
        doc_info = loader.get_document_info()
        
        if doc_info['supported_files'] == 0:
            logger.info("æœªæ‰¾åˆ°æ–‡æ¡£ï¼Œåˆ›å»ºç¤ºä¾‹çŸ¥è¯†åº“...")
            
            # åˆ›å»ºç¤ºä¾‹æ–‡æ¡£
            sample_docs = [
                "æ·±è¹²æ˜¯åŠ›é‡ä¸¾ä¸‰å¤§é¡¹ä¹‹ä¸€ï¼Œä¸»è¦é”»ç‚¼ä¸‹è‚¢åŠ›é‡ã€‚æ­£ç¡®çš„æ·±è¹²å§¿åŠ¿åŒ…æ‹¬ï¼šåŒè„šä¸è‚©åŒå®½ï¼Œè„šå°–ç•¥å¾®å¤–å±•ï¼Œä¸‹è¹²æ—¶è†ç›–ä¸è¶…è¿‡è„šå°–ï¼Œä¿æŒèƒŒéƒ¨æŒºç›´ã€‚",
                "ç¡¬æ‹‰æ˜¯é”»ç‚¼å…¨èº«åŠ›é‡çš„é‡è¦åŠ¨ä½œã€‚èµ·å§‹å§¿åŠ¿ï¼šåŒè„šä¸è‚©åŒå®½ï¼ŒåŒæ‰‹æ¡æ é“ƒï¼ŒèƒŒéƒ¨æŒºç›´ï¼Œè‡€éƒ¨ä¸‹æ²‰ã€‚æ‹‰èµ·æ—¶ä¿æŒèƒŒéƒ¨æŒºç›´ï¼Œç”¨è…¿éƒ¨åŠ›é‡å¯åŠ¨ã€‚",
                "å§æ¨ä¸»è¦é”»ç‚¼èƒ¸éƒ¨å’Œä¸Šè‚¢åŠ›é‡ã€‚èººåœ¨å§æ¨å‡³ä¸Šï¼ŒåŒè„šå¹³æ”¾åœ°é¢ï¼ŒåŒæ‰‹æ¡æ é“ƒï¼Œä¸‹æ”¾æ—¶æ§åˆ¶é€Ÿåº¦ï¼Œæ¨èµ·æ—¶å‘¼æ°”ã€‚æ³¨æ„è‚©èƒ›éª¨æ”¶ç´§ã€‚",
                "åŠ›é‡ä¸¾è®­ç»ƒçš„åŸºæœ¬åŸåˆ™ï¼šæ¸è¿›è¶…è´Ÿè·ï¼Œå……åˆ†ä¼‘æ¯ï¼Œæ­£ç¡®æŠ€æœ¯ï¼Œåˆç†è¥å…»ã€‚å»ºè®®æ¯å‘¨è®­ç»ƒ3-4æ¬¡ï¼Œæ¯æ¬¡1-2å°æ—¶ã€‚",
                "è®­ç»ƒå®‰å…¨æ³¨æ„äº‹é¡¹ï¼šå……åˆ†çƒ­èº«ï¼Œä½¿ç”¨æ­£ç¡®çš„é‡é‡ï¼Œä¿æŒæ­£ç¡®å§¿åŠ¿ï¼Œä¸è¦æ€¥äºå¢åŠ é‡é‡ã€‚å¦‚æœ‰ä¸é€‚ç«‹å³åœæ­¢è®­ç»ƒã€‚"
            ]
            
            # åˆ›å»ºæ–‡æ¡£å¯¹è±¡
            from langchain.schema import Document
            documents = []
            for i, text in enumerate(sample_docs):
                doc = Document(
                    page_content=text,
                    metadata={
                        'source': 'sample_data',
                        'file_name': f'sample_{i+1}.txt',
                        'file_type': '.txt'
                    }
                )
                documents.append(doc)
            
            # å¤„ç†æ–‡æ¡£
            processor = TextProcessor()
            processed_docs = processor.process_documents(documents)
            
            # æ·»åŠ åˆ°çŸ¥è¯†åº“
            agent = RAGAgent()
            success = agent.add_documents(processed_docs)
            
            if success:
                logger.info("ç¤ºä¾‹çŸ¥è¯†åº“åˆ›å»ºæˆåŠŸ")
            else:
                logger.error("ç¤ºä¾‹çŸ¥è¯†åº“åˆ›å»ºå¤±è´¥")
                
        else:
            logger.info(f"æ‰¾åˆ° {doc_info['supported_files']} ä¸ªæ–‡æ¡£")
            
    except Exception as e:
        logger.error(f"åˆ›å»ºç¤ºä¾‹æ•°æ®å¤±è´¥: {e}")

def run_streamlit_app():
    """è¿è¡ŒStreamlitåº”ç”¨"""
    try:
        from ui.chat_interface import ChatInterface
        
        # åˆ›å»ºå¹¶è¿è¡ŒèŠå¤©ç•Œé¢
        chat_interface = ChatInterface()
        chat_interface.run()
        
    except Exception as e:
        logger.error(f"è¿è¡ŒStreamlitåº”ç”¨å¤±è´¥: {e}")
        raise

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ‹ï¸ BarbellGPT - åŠ›é‡ä¸¾è®­ç»ƒæ™ºèƒ½åŠ©æ‰‹")
    print("=" * 50)
    
    # è®¾ç½®æ—¥å¿—
    setup_logging()
    logger.info("BarbellGPT å¯åŠ¨ä¸­...")
    
    # æ£€æŸ¥ä¾èµ–
    if not check_dependencies():
        sys.exit(1)
    
    # æ£€æŸ¥é…ç½®
    check_config()
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    create_sample_data()
    
    # è¿è¡Œåº”ç”¨
    logger.info("å¯åŠ¨Streamlitåº”ç”¨...")
    run_streamlit_app()

if __name__ == "__main__":
    main() 